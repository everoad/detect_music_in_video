import tensorflow as tf
import tensorflow_hub as hub
import librosa
import matplotlib.pyplot as plt
import numpy as np
import imageio_ffmpeg as ffmpeg
import subprocess
import scipy.signal as signal
import noisereduce as nr
import csv
from scipy.ndimage import gaussian_filter1d
import json
import itertools
from sklearn.cluster import DBSCAN

sample_rate = 16000

# ë‚´ë¶€ ì—°ì‚°(ì˜ˆ: í–‰ë ¬ ê³±ì…ˆ)ì—ì„œ ì‚¬ìš©í•  ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
tf.config.threading.set_intra_op_parallelism_threads(2)
# ì—°ì‚° ê°„ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
tf.config.threading.set_inter_op_parallelism_threads(2)

def classes_from_csv(class_map_csv_text):
  """Returns list of class names corresponding to score vector."""
  class_names = []
  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
      class_names.append([row['display_name'], int(row['index'])])

  return class_names


def extract_audio(video_path, audio_path):
    """ FFmpegì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ """
    ffmpeg_path = ffmpeg.get_ffmpeg_exe()
    command = [ffmpeg_path, "-i", video_path, "-acodec", "pcm_s16le", "-ar", f"{sample_rate}", "-ac", "1", audio_path, "-y"]
    subprocess.run(command, shell=True)


def group_and_filter_music_times(music_times, min_term_duration):
    grouped_music_times = []
    current_group = []
    for time in music_times:
        if not current_group:
            current_group.append(time)
        elif (time - current_group[-1]) < min_term_duration:
            current_group.append(time)
        else:
            grouped_music_times.append(current_group)
            current_group = [time]
    if current_group:
        grouped_music_times.append(current_group)

    filtered_music_times = []
    for group in grouped_music_times:
        start_time, end_time = min(group), max(group)
        if (end_time - start_time) >= (min_term_duration * 0.5) and (end_time - start_time)/len(group) <= 1:
            print(f"êµ¬ê°„ ì ìˆ˜: {((end_time - start_time)/len(group)):.3f}")
            filtered_music_times.append(group)
    
    return filtered_music_times

def iterative_group_and_filter(music_times, min_durations):
    """
    min_durations ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê°’ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ë©´ì„œ ìŒì•… ì‹œê°„ì„ ê·¸ë£¹í™”í•˜ê³  í•„í„°ë§í•œë‹¤.
    """
    grouped_music_times = music_times
    for index, min_duration in enumerate(min_durations):
        if (index == 0):
            grouped_music_times = group_and_filter_music_times(grouped_music_times, min_duration)
        else:
            flattened = list(itertools.chain.from_iterable(grouped_music_times))
            grouped_music_times = group_and_filter_music_times(flattened, min_duration)

    return grouped_music_times


def detect_music_sections(audio_path):
    """ YAMNet ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì•… êµ¬ê°„ íƒì§€ """
    model = hub.load("https://tfhub.dev/google/yamnet/1")

    class_map_path = model.class_map_path().numpy()
    classes = classes_from_csv(class_map_path)
    music_class_index = [index for name, index in classes if "Music" in name][0]
    print(f"Music class index: {music_class_index}")
    singing_class_index = [index for name, index in classes if "Singing" in name][0]
    print(f"Singing class index: {singing_class_index}")

    y, sr = librosa.load(audio_path, sr=sample_rate)
    # y = reduce_noise(y, sampleRate)
    y = reduce_noise_chunked(y, sample_rate, chunk_duration=10)
    y = apply_bandpass_filter(y, sample_rate)
    y = np.array(y, dtype=np.float32)
    

    print("ì²­í¬ ë‹¨ìœ„ë¡œ ëª¨ë¸ ë¶„ì„ ì‹œì‘...")
    chunk_duration = 10  # ì´ˆ
    chunk_samples = int(chunk_duration * sample_rate)
    all_scores = []
    for start in segment(0, len(y), chunk_samples):
        end = start + chunk_samples
        chunk = y[start:end]
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ 10ì´ˆë³´ë‹¤ ì§§ìœ¼ë©´ 0 íŒ¨ë”©
        if len(chunk) < chunk_samples:
            chunk = np.pad(chunk, (0, chunk_samples - len(chunk)), mode='constant')
        chunk_scores, chunk_embeddings, chunk_spectrogram = model(chunk)
        all_scores.append(chunk_scores)
    
     # ì²­í¬ë³„ ê²°ê³¼ ê²°í•©
    scores_np = np.concatenate([s.numpy() for s in all_scores], axis=0)
    print("ëª¨ë¸ ë¶„ì„ ì™„ë£Œ.")
    
    # Musics, Singing í´ë˜ìŠ¤ì˜ í™•ë¥ ë§Œ ì¶”ì¶œ
    music_prob_smoothed = gaussian_filter1d(scores_np[:, music_class_index], sigma=3)
    singing_prob_smoothed = gaussian_filter1d(scores_np[:, singing_class_index], sigma=3)

    # ë™ì  ì„ê³„ê°’
    music_dynamic_threshold = np.mean(music_prob_smoothed)
    singing_dynamic_threshold = np.mean(singing_prob_smoothed)
    print(f"Music ë™ì  ì„ê³„ê°’: {music_dynamic_threshold}")
    print(f"Singing ë™ì  ì„ê³„ê°’: {singing_dynamic_threshold}")
    
    # í”„ë ˆì„ë³„ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì°¾ê¸° (Top-1)
    top_class_indices = np.argmax(scores_np, axis=1)

    duration = len(y)/sample_rate
    frame_duration = duration / len(top_class_indices)
    timestamps = np.arange(len(top_class_indices)) * frame_duration

    # music ë˜ëŠ” singingì´ ìµœê³  í™•ë¥ ì¸ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
    music_or_singing_indices = np.where(
        (top_class_indices == music_class_index) | (top_class_indices == singing_class_index)
    )[0]
    music_or_singing_times = timestamps[music_or_singing_indices]

    # plot_music_probability(music_prob_smoothed, frame_duration, dynamic_threshold)

    # min_duration ì´ìƒì¸ êµ¬ê°„ ì œê±°
    diff = np.diff(music_or_singing_times)
    mask = diff < (frame_duration * 5)
    # ì²« ë²ˆì§¸ ê°’ì€ ì¡°ê±´ ë¹„êµì—ì„œ ì œì™¸ë˜ë¯€ë¡œ, maskë¥¼ prepend Falseí•˜ì—¬ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìŒ
    filtered_music_or_singing_times = music_or_singing_times[np.concatenate(([False], mask))]

    # ì¸ì ‘í•œ ê°’ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
    min_term_durations = [7.5, 10, 15]
    grouped_music_or_singing_times = iterative_group_and_filter(filtered_music_or_singing_times, min_term_durations)

    # ê° êµ¬ê°„ì˜ í‰ê·  Music í™•ë¥  ê³„ì‚°
    section_averages = []
    for group in grouped_music_or_singing_times:
        start_time, end_time = min(group), max(group)
        
        # í•´ë‹¹ ì‹œê°„ ë²”ìœ„ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        section_indices = np.where((timestamps >= start_time) & (timestamps <= end_time))[0]
        
        if len(section_indices) > 0:
            section_averages.append([
                np.mean(music_prob_smoothed[section_indices]),  # í‰ê·  Music í™•ë¥  ê³„ì‚°
                np.mean(singing_prob_smoothed[section_indices]) # í‰ê·  Singing í™•ë¥  ê³„ì‚°
            ])
        else:
            section_averages.append([0, 0])

    for group, avg_score in zip(grouped_music_or_singing_times, section_averages):
        start_time, end_time = min(group), max(group)
        group_size = len(group)
        print(f"ìµœì¢… êµ¬ê°„ ì ìˆ˜: {start_time:.0f} ~ {end_time:.0f}, {(end_time - start_time):.0f}, {avg_score[0]:.2f}, {avg_score[1]:.2f}, {((end_time - start_time)/group_size):.2f}")


    exception_high_factor = 1.5
    exception_row_factor = 0.5
    min_group_duration = 60
    #ğŸ¯ 30ì´ˆ ì´ìƒ & í‰ê·  Music í™•ë¥ ì´ 0.35 ì´ìƒì¸ êµ¬ê°„ë§Œ í¬í•¨
    final_grouped_music_times = [
        group for group, avg_score in zip(grouped_music_or_singing_times, section_averages)
        if (max(group) - min(group)) > min_group_duration and (
            (avg_score[0] >= music_dynamic_threshold and avg_score[1] >= singing_dynamic_threshold) or
            (avg_score[0] >= music_dynamic_threshold * exception_high_factor and avg_score[1] >= singing_dynamic_threshold * exception_row_factor) or
            (avg_score[0] >= music_dynamic_threshold * exception_row_factor and avg_score[1] >= singing_dynamic_threshold * exception_high_factor)
        )
    ]
    return final_grouped_music_times

def find_music_segments(video_file):
    """ ë™ì˜ìƒì—ì„œ ìŒì•… êµ¬ê°„ì„ íƒì§€í•˜ê³  ë°˜í™˜ """
    audio_path = "C:/Users/kbj/Downloads/temp_audio2.wav"
    extract_audio(video_file, audio_path)
    return detect_music_sections(audio_path)


def reduce_noise(y, sr):
    """ ë°°ê²½ ì†ŒìŒ ì œê±° """
    auto_prop_decrease = detect_auto_prop_decrease(y)
    return nr.reduce_noise(y=y, sr=sr, prop_decrease=auto_prop_decrease, stationary=True)

def reduce_noise_chunked(y, sr, chunk_duration=10):
    """ ì²­í¬ ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ ì œê±° ìˆ˜í–‰ (ê¸°ë³¸ ì²­í¬ ê¸¸ì´: 10ì´ˆ) """
    print("ë…¸ì´ì¦ˆ ì œê±° ì‹œì‘ (ì²­í¬ ë‹¨ìœ„)...")
    chunk_size = int(chunk_duration * sr)  # ì²­í¬ë‹¹ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    auto_prop_decrease = detect_auto_prop_decrease(y)
    reduced_audio = np.empty_like(y)
    # ì²­í¬ë³„ë¡œ ë…¸ì´ì¦ˆ ì œê±° ì²˜ë¦¬
    for start in segment(0, len(y), chunk_size):
        end = min(start + chunk_size, len(y))
        chunk = y[start:end]
        reduced_chunk = nr.reduce_noise(y=chunk, sr=sr, prop_decrease=auto_prop_decrease, stationary=True)
        reduced_audio[start:end] = reduced_chunk
    print("ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ.")
    return reduced_audio


def detect_auto_prop_decrease(y):
    """ ë…¸ì´ì¦ˆ ë ˆë²¨(SNR)ì— ë”°ë¼ prop_decrease ê°’ ìë™ ì¡°ì • """
    noise_level = np.mean(np.abs(y))  # ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ í‰ê·  ì§„í­ ê³„ì‚°
    print(f"ë…¸ì´ì¦ˆ ë ˆë²¨: {noise_level}")
    if noise_level < 0.01:  # ë§¤ìš° ì¡°ìš©í•œ ì˜¤ë””ì˜¤
        return 0.2
    elif noise_level < 0.05:  # ì¤‘ê°„ ì •ë„ ì†ŒìŒ
        return 0.4
    else:  # ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ê²½ìš°
        return 0.6
    

def apply_bandpass_filter(y, sr, lowcut=200, highcut=8000, order=5):
    """ ì €ì£¼íŒŒ(200Hz ì´í•˜) & ê³ ì£¼íŒŒ(8kHz ì´ìƒ) ì œê±° """
    print("ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì‹œì‘...")
    nyquist = 0.5 * sr
    if highcut >= nyquist:
        highcut = nyquist - 1
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = signal.butter(order, [low, high], btype='band')
    result = signal.lfilter(b, a, y)
    print("ë°´ë“œíŒ¨ìŠ¤ í•„í„° ì™„ë£Œ.")
    return result


def plot_music_probability(music_prob, frame_duration, threshold):
    """ music_probì„ ì‹œê°„ì— ë”°ë¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” """
    # Xì¶•: ì‹œê°„(ì´ˆ)
    time_axis = np.arange(len(music_prob)) * frame_duration
    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(12, 5))
    plt.plot(time_axis, music_prob, label="Music Probability", color="b", alpha=0.7)
    plt.axhline(y=threshold, color='r', linestyle='--', label=f"Threshold ({threshold})")  # ì„ê³„ê°’ í‘œì‹œ
    plt.xlabel("Time (seconds)")
    plt.ylabel("Probability of Music")
    plt.title("Music Probability over Time")
    plt.legend()
    plt.grid(True)
    plt.show()


# ì‹¤í–‰ ì˜ˆì œ
video_file = "C:/Users/kbj/Downloads/videoplayback3.mp4"
segments = find_music_segments(video_file)

ranges = []
for index, segment in enumerate(segments):
    start_time, end_time = min(segment), max(segment)
    ranges.append({
        "start": start_time,  # êµ¬ê°„ì˜ ìµœì†Œê°’
        "end": end_time     # êµ¬ê°„ì˜ ìµœëŒ€ê°’
    })
    print(f"ë…¸ë˜ {index + 1}: {start_time:.0f} ~ {end_time:.0f}")  # ì›ë˜ ì¶œë ¥ ìœ ì§€

# JSON íŒŒì¼ë¡œ ì €ì¥
with open('ranges.json', 'w', encoding='utf-8') as f:
    json.dump(ranges, f, indent=2)

print("JSON file 'ranges.json' created successfully.")