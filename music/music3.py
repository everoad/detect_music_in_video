import tensorflow as tf
import tensorflow_hub as hub
import librosa
import matplotlib.pyplot as plt
import numpy as np
import imageio_ffmpeg as ffmpeg
import subprocess
import scipy.signal as signal
import noisereduce as nr
import csv
from scipy.ndimage import gaussian_filter1d

sampleRate = 16000

def classes_from_csv(class_map_csv_text):
  """Returns list of class names corresponding to score vector."""
  class_names = []
  with tf.io.gfile.GFile(class_map_csv_text) as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
      class_names.append([row['display_name'], int(row['index'])])

  return class_names


def extract_audio(video_path, audio_path):
    """ FFmpegì„ ì‚¬ìš©í•˜ì—¬ ë™ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ """
    ffmpeg_path = ffmpeg.get_ffmpeg_exe()
    command = [ffmpeg_path, "-i", video_path, "-acodec", "pcm_s16le", "-ar", f"{sampleRate}", "-ac", "1", audio_path, "-y"]
    subprocess.run(command, shell=True)


def group_and_filter_music_times(music_times, min_term_duration):
    grouped_music_times = []
    current_group = []
    for time in music_times:
        if not current_group:
            current_group.append(time)
        elif (time - current_group[-1]) < min_term_duration:
            current_group.append(time)
        else:
            grouped_music_times.append(current_group)
            current_group = [time]
    if current_group:
        grouped_music_times.append(current_group)

    filtered_music_times = [
        group for group in grouped_music_times
        if (max(group) - min(group)) >= (min_term_duration * 0.5)
    ]

    return filtered_music_times

def iterative_group_and_filter(music_times, min_durations):
    """
    min_durations ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê°’ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ë©´ì„œ ìŒì•… ì‹œê°„ì„ ê·¸ë£¹í™”í•˜ê³  í•„í„°ë§í•œë‹¤.
    """
    grouped_music_times = music_times
    for index, min_duration in enumerate(min_durations):
        if (index == 0):
            grouped_music_times = group_and_filter_music_times(grouped_music_times, min_duration)
        else:
            grouped_music_times = group_and_filter_music_times(sum(grouped_music_times, []), min_duration)

    return grouped_music_times

def detect_music_sections(audio_path):
    """ YAMNet ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì•… êµ¬ê°„ íƒì§€ """
    model = hub.load("https://tfhub.dev/google/yamnet/1")

    class_map_path = model.class_map_path().numpy()
    classes = classes_from_csv(class_map_path)
    music_class_index = [index for name, index in classes if "Music" in name][0]
    print(f"Music class index: {music_class_index}")

    y, sr = librosa.load(audio_path, sr=sampleRate)
    y = reduce_noise(y, sampleRate)
    y = apply_bandpass_filter(y, sampleRate)
    y = np.array(y, dtype=np.float32)
    
    duration = len(y)/sampleRate
    scores, embeddings, spectrogram = model(y)

    # Music í´ë˜ìŠ¤ì˜ í™•ë¥ ë§Œ ì¶”ì¶œ
    music_prob = scores.numpy()[:, music_class_index]
    music_prob_smoothed = gaussian_filter1d(music_prob, sigma=3)

    dynamic_threshold = np.mean(music_prob_smoothed)
    # í”„ë ˆì„ë³„ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì°¾ê¸° (Top-1)
    top_class_indices = np.argmax(scores.numpy(), axis=1)

    frame_duration = duration / len(top_class_indices)
    timestamps = np.array([i * frame_duration for i in range(len(scores))])

    music_indices = np.where(top_class_indices == music_class_index)[0]
    music_times_in_seconds = timestamps[music_indices]

    plot_music_probability(music_prob_smoothed, frame_duration, dynamic_threshold)
    # min_duration ì´ìƒì¸ êµ¬ê°„ ì œê±°
    filtered_music_times = []
    prev_time = None
    for time in music_times_in_seconds:
        if prev_time is not None and (time - prev_time < frame_duration * 3):
            filtered_music_times.append(time)
        prev_time = time

    # ì¸ì ‘í•œ ê°’ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
    min_term_durations = [5, 10, 15]
    grouped_music_times = iterative_group_and_filter(filtered_music_times, min_term_durations)

    # ê° êµ¬ê°„ì˜ í‰ê·  Music í™•ë¥  ê³„ì‚°
    music_section_averages = []
    for group in grouped_music_times:
        start_time, end_time = min(group), max(group)
        
        # í•´ë‹¹ ì‹œê°„ ë²”ìœ„ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        section_indices = np.where((timestamps >= start_time) & (timestamps <= end_time))[0]
        
        if len(section_indices) > 0:
            avg_music_score = np.mean(music_prob_smoothed[section_indices])  # í‰ê·  Music í™•ë¥  ê³„ì‚°
        else:
            avg_music_score = 0  # ë§Œì•½ êµ¬ê°„ ë‚´ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 0 ì²˜ë¦¬
        
        music_section_averages.append(avg_music_score)

    for group, avg_score in zip(grouped_music_times, music_section_averages):
        print(f"{min(group):.1f} ~ {max(group):.1f}, {avg_score:.2f}")

    min_group_duration = 60
    #ğŸ¯ 30ì´ˆ ì´ìƒ & í‰ê·  Music í™•ë¥ ì´ 0.35 ì´ìƒì¸ êµ¬ê°„ë§Œ í¬í•¨
    final_grouped_music_times = [
        group for group, avg_score in zip(grouped_music_times, music_section_averages)
        if (max(group) - min(group)) > min_group_duration and avg_score >= dynamic_threshold
    ]
    return final_grouped_music_times


def find_music_segments(video_file):
    """ ë™ì˜ìƒì—ì„œ ìŒì•… êµ¬ê°„ì„ íƒì§€í•˜ê³  ë°˜í™˜ """
    audio_path = "C:/Users/kbj/Downloads/temp_audio.wav"
    extract_audio(video_file, audio_path)
    return detect_music_sections(audio_path)


def reduce_noise(y, sr):
    """ ë°°ê²½ ì†ŒìŒ ì œê±° """
    auto_prop_decrease = detect_auto_prop_decrease(y)
    return nr.reduce_noise(y=y, sr=sr, prop_decrease=auto_prop_decrease, stationary=True)


def detect_auto_prop_decrease(y):
    """ ë…¸ì´ì¦ˆ ë ˆë²¨(SNR)ì— ë”°ë¼ prop_decrease ê°’ ìë™ ì¡°ì • """
    noise_level = np.mean(np.abs(y))  # ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ í‰ê·  ì§„í­ ê³„ì‚°
    print(f"Noise level: {noise_level}")
    if noise_level < 0.01:  # ë§¤ìš° ì¡°ìš©í•œ ì˜¤ë””ì˜¤
        return 0.2
    elif noise_level < 0.05:  # ì¤‘ê°„ ì •ë„ ì†ŒìŒ
        return 0.4
    else:  # ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ê²½ìš°
        return 0.6
    

def apply_bandpass_filter(y, sr, lowcut=200, highcut=8000, order=5):
    """ ì €ì£¼íŒŒ(200Hz ì´í•˜) & ê³ ì£¼íŒŒ(8kHz ì´ìƒ) ì œê±° """
     # í•„í„° ê³„ìˆ˜ ê³„ì‚° (Butterworth í•„í„°)
    nyquist = 0.5 * sr  # ë‚˜ì´í€´ìŠ¤íŠ¸ ì£¼íŒŒìˆ˜ (ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ì˜ ì ˆë°˜)

    if highcut >= nyquist:
        highcut = nyquist - 1
    
    low = lowcut / nyquist
    high = highcut / nyquist

    b, a = signal.butter(order, [low, high], btype='band')  # Bandpass í•„í„° ìƒì„±

    # í•„í„° ì ìš©
    return signal.lfilter(b, a, y)


def plot_music_probability(music_prob, frame_duration, threshold):
    """ music_probì„ ì‹œê°„ì— ë”°ë¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™” """
    # Xì¶•: ì‹œê°„(ì´ˆ)
    time_axis = np.arange(len(music_prob)) * frame_duration
    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(12, 5))
    plt.plot(time_axis, music_prob, label="Music Probability", color="b", alpha=0.7)
    plt.axhline(y=threshold, color='r', linestyle='--', label=f"Threshold ({threshold})")  # ì„ê³„ê°’ í‘œì‹œ
    plt.xlabel("Time (seconds)")
    plt.ylabel("Probability of Music")
    plt.title("Music Probability over Time")
    plt.legend()
    plt.grid(True)
    plt.show()


# ì‹¤í–‰ ì˜ˆì œ
video_file = "C:/Users/kbj/Downloads/videoplayback2.mp4"
section = find_music_segments(video_file)
for idx, range in enumerate(section):
    print(f"music {idx + 1}: {min(range)} ~ {max(range)}")